{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","# Azure storage access info\n","blob_account_name = \"fabrictrainingcoursesa\"\n","blob_container_name = \"coursedata\"\n","blob_relative_path = \"instacart-order_products__prior.csv\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","\n","print('Remote blob path: ' + wasbs_path)\n","\n","# SPARK read parquet/CSV, note that it won't load any data yet by now\n","#df = spark.read.parquet(wasbs_path)\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\n","print('Register the DataFrame as a SQL temporary view: source')\n","df.createOrReplaceTempView('source')\n","\n","# Display top 10 rows\n","print('Displaying top 10 rows: ')\n","display(spark.sql('SELECT * FROM source LIMIT 10'))\n","\n","# Save the dataframe into a parquet file to our data lakehouse\n","#df.write.mode(\"overwrite\").parquet('Files/nyctlc/fhv')\n","\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"instacart_order_products_prior\", path=\"abfss://b12ab912-c7ea-4752-9acf-910618765439@onelake.dfs.fabric.microsoft.com/a9f02573-715a-45b6-be63-4e5f8035a3ce/Files/Groceries/instacart_order_products_prior\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"2050ba22-a5da-4318-9b67-74fd51a9cf1f","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-13T23:24:57.4605707Z","session_start_time":"2023-06-13T23:24:57.7942323Z","execution_start_time":"2023-06-13T23:24:57.9596324Z","execution_finish_time":"2023-06-13T23:25:26.4960873Z","spark_jobs":{"numbers":{"SUCCEEDED":8,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4444,"rowCount":50,"jobId":54,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:23.137GMT","completionTime":"2023-06-13T23:25:23.165GMT","stageIds":[74,75,73],"jobGroup":"8","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4444,"dataRead":5795,"rowCount":61,"jobId":53,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:22.604GMT","completionTime":"2023-06-13T23:25:23.124GMT","stageIds":[71,72],"jobGroup":"8","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":5795,"dataRead":5447,"rowCount":22,"jobId":52,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:22.423GMT","completionTime":"2023-06-13T23:25:22.472GMT","stageIds":[70],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":51,"name":"","description":"Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:21.953GMT","completionTime":"2023-06-13T23:25:21.953GMT","stageIds":[],"jobGroup":"8","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":139014138,"dataRead":578207766,"rowCount":64868978,"jobId":50,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:00.925GMT","completionTime":"2023-06-13T23:25:21.831GMT","stageIds":[69],"jobGroup":"8","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":49,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:24:58.638GMT","completionTime":"2023-06-13T23:24:58.656GMT","stageIds":[67,68],"jobGroup":"8","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2323,"dataRead":524288,"rowCount":160,"jobId":48,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:24:58.299GMT","completionTime":"2023-06-13T23:24:58.624GMT","stageIds":[66],"jobGroup":"8","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":47,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__prior.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:24:58.054GMT","completionTime":"2023-06-13T23:24:58.208GMT","stageIds":[65],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"8ac4b76a-7600-4700-bd06-16512809afcc"},"text/plain":"StatementMeta(, 2050ba22-a5da-4318-9b67-74fd51a9cf1f, 8, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Remote blob path: wasbs://coursedata@fabrictrainingcoursesa.blob.core.windows.net/instacart-order_products__prior.csv\nRegister the DataFrame as a SQL temporary view: source\nDisplaying top 10 rows: \n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"9706acd9-4411-4292-9753-c5377c776304","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 9706acd9-4411-4292-9753-c5377c776304)"},"metadata":{}}],"execution_count":6,"metadata":{"collapsed":false}},{"cell_type":"code","source":["# Welcome to your new notebook\r\n","# Type here in the cell editor to add code!\r\n","# Azure storage access info\r\n","blob_account_name = \"fabrictrainingcoursesa\"\r\n","blob_container_name = \"coursedata\"\r\n","blob_relative_path = \"instacart-aisles.csv\"\r\n","blob_sas_token = r\"\"\r\n","\r\n","# Allow SPARK to read from Blob remotely\r\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n","spark.conf.set(\r\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n","  blob_sas_token)\r\n","\r\n","print('Remote blob path: ' + wasbs_path)\r\n","\r\n","# SPARK read parquet/CSV, note that it won't load any data yet by now\r\n","#df = spark.read.parquet(wasbs_path)\r\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\r\n","print('Register the DataFrame as a SQL temporary view: source')\r\n","df.createOrReplaceTempView('source')\r\n","\r\n","# Display top 10 rows\r\n","print('Displaying top 10 rows: ')\r\n","display(spark.sql('SELECT * FROM source LIMIT 10'))\r\n","\r\n","# Save the dataframe into a parquet file to our data lakehouse\r\n","#df.write.mode(\"overwrite\").parquet('Files/nyctlc/fhv')\r\n","\r\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"instacart_aisles\", path=\"abfss://b12ab912-c7ea-4752-9acf-910618765439@onelake.dfs.fabric.microsoft.com/a9f02573-715a-45b6-be63-4e5f8035a3ce/Files/Groceries/instacart_aisles\")\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"2050ba22-a5da-4318-9b67-74fd51a9cf1f","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-13T23:24:57.4702058Z","session_start_time":null,"execution_start_time":"2023-06-13T23:25:26.8693712Z","execution_finish_time":"2023-06-13T23:25:30.6781724Z","spark_jobs":{"numbers":{"SUCCEEDED":8,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4309,"rowCount":50,"jobId":62,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:29.042GMT","completionTime":"2023-06-13T23:25:29.069GMT","stageIds":[84,85,86],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4309,"dataRead":1561,"rowCount":54,"jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:28.596GMT","completionTime":"2023-06-13T23:25:29.026GMT","stageIds":[82,83],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1561,"dataRead":1299,"rowCount":8,"jobId":60,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:28.439GMT","completionTime":"2023-06-13T23:25:28.487GMT","stageIds":[81],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":59,"name":"","description":"Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:27.987GMT","completionTime":"2023-06-13T23:25:27.987GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3895,"dataRead":3388,"rowCount":268,"jobId":58,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:27.657GMT","completionTime":"2023-06-13T23:25:27.897GMT","stageIds":[80],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":387,"rowCount":10,"jobId":57,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:27.198GMT","completionTime":"2023-06-13T23:25:27.209GMT","stageIds":[78,79],"jobGroup":"9","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":387,"dataRead":2603,"rowCount":20,"jobId":56,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:27.140GMT","completionTime":"2023-06-13T23:25:27.189GMT","stageIds":[77],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2603,"rowCount":1,"jobId":55,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-aisles.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:26.970GMT","completionTime":"2023-06-13T23:25:27.051GMT","stageIds":[76],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"33cbc2e9-4b40-4f74-8512-ace30609551f"},"text/plain":"StatementMeta(, 2050ba22-a5da-4318-9b67-74fd51a9cf1f, 9, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Remote blob path: wasbs://coursedata@fabrictrainingcoursesa.blob.core.windows.net/instacart-aisles.csv\nRegister the DataFrame as a SQL temporary view: source\nDisplaying top 10 rows: \n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"07afe3cb-b851-4a1e-bde4-ef74fa0e6c1f","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 07afe3cb-b851-4a1e-bde4-ef74fa0e6c1f)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false}},{"cell_type":"code","source":["# Welcome to your new notebook\r\n","# Type here in the cell editor to add code!\r\n","# Azure storage access info\r\n","blob_account_name = \"fabrictrainingcoursesa\"\r\n","blob_container_name = \"coursedata\"\r\n","blob_relative_path = \"instacart-order_products__train.csv\"\r\n","blob_sas_token = r\"\"\r\n","\r\n","# Allow SPARK to read from Blob remotely\r\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n","spark.conf.set(\r\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n","  blob_sas_token)\r\n","\r\n","print('Remote blob path: ' + wasbs_path)\r\n","\r\n","# SPARK read parquet/CSV, note that it won't load any data yet by now\r\n","#df = spark.read.parquet(wasbs_path)\r\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\r\n","print('Register the DataFrame as a SQL temporary view: source')\r\n","df.createOrReplaceTempView('source')\r\n","\r\n","# Display top 10 rows\r\n","print('Displaying top 10 rows: ')\r\n","display(spark.sql('SELECT * FROM source LIMIT 10'))\r\n","\r\n","# Save the dataframe into a parquet file to our data lakehouse\r\n","#df.write.mode(\"overwrite\").parquet('Files/nyctlc/fhv')\r\n","\r\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"instacart_order_products_train\", path=\"abfss://b12ab912-c7ea-4752-9acf-910618765439@onelake.dfs.fabric.microsoft.com/a9f02573-715a-45b6-be63-4e5f8035a3ce/Files/Groceries/instacart_order_products_train\")\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"2050ba22-a5da-4318-9b67-74fd51a9cf1f","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-13T23:24:57.4831571Z","session_start_time":null,"execution_start_time":"2023-06-13T23:25:31.0436831Z","execution_finish_time":"2023-06-13T23:25:36.2215076Z","spark_jobs":{"numbers":{"SUCCEEDED":8,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4396,"rowCount":50,"jobId":70,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:34.713GMT","completionTime":"2023-06-13T23:25:34.740GMT","stageIds":[96,97,95],"jobGroup":"10","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4396,"dataRead":4586,"rowCount":59,"jobId":69,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:34.341GMT","completionTime":"2023-06-13T23:25:34.697GMT","stageIds":[93,94],"jobGroup":"10","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4586,"dataRead":4332,"rowCount":18,"jobId":68,"name":"toString at String.java:2994","description":"Delta: Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:34.184GMT","completionTime":"2023-06-13T23:25:34.230GMT","stageIds":[92],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":67,"name":"","description":"Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:33.739GMT","completionTime":"2023-06-13T23:25:33.739GMT","stageIds":[],"jobGroup":"10","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":6314922,"dataRead":25015579,"rowCount":2769234,"jobId":66,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:32.180GMT","completionTime":"2023-06-13T23:25:33.635GMT","stageIds":[91],"jobGroup":"10","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":65,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:31.699GMT","completionTime":"2023-06-13T23:25:31.716GMT","stageIds":[89,90],"jobGroup":"10","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1733,"dataRead":393216,"rowCount":120,"jobId":64,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:31.444GMT","completionTime":"2023-06-13T23:25:31.684GMT","stageIds":[88],"jobGroup":"10","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":63,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-order_products__train.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our...","submissionTime":"2023-06-13T23:25:31.114GMT","completionTime":"2023-06-13T23:25:31.361GMT","stageIds":[87],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c95fbe08-8606-4c1a-a63e-66246a09219f"},"text/plain":"StatementMeta(, 2050ba22-a5da-4318-9b67-74fd51a9cf1f, 10, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Remote blob path: wasbs://coursedata@fabrictrainingcoursesa.blob.core.windows.net/instacart-order_products__train.csv\nRegister the DataFrame as a SQL temporary view: source\nDisplaying top 10 rows: \n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"20ee5d39-8420-4b4c-9171-315e0e8f3d4d","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 20ee5d39-8420-4b4c-9171-315e0e8f3d4d)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false}},{"cell_type":"code","source":["# Welcome to your new notebook\r\n","# Type here in the cell editor to add code!\r\n","# Azure storage access info\r\n","blob_account_name = \"fabrictrainingcoursesa\"\r\n","blob_container_name = \"coursedata\"\r\n","blob_relative_path = \"instacart-orders.csv\"\r\n","blob_sas_token = r\"\"\r\n","\r\n","# Allow SPARK to read from Blob remotely\r\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n","spark.conf.set(\r\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n","  blob_sas_token)\r\n","\r\n","print('Remote blob path: ' + wasbs_path)\r\n","\r\n","# SPARK read parquet/CSV, note that it won't load any data yet by now\r\n","#df = spark.read.parquet(wasbs_path)\r\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\r\n","print('Register the DataFrame as a SQL temporary view: source')\r\n","df.createOrReplaceTempView('source')\r\n","\r\n","# Display top 10 rows\r\n","print('Displaying top 10 rows: ')\r\n","display(spark.sql('SELECT * FROM source LIMIT 10'))\r\n","\r\n","# Save the dataframe into a parquet file to our data lakehouse\r\n","#df.write.mode(\"overwrite\").parquet('Files/nyctlc/fhv')\r\n","\r\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"instacart_orders\", path=\"abfss://b12ab912-c7ea-4752-9acf-910618765439@onelake.dfs.fabric.microsoft.com/a9f02573-715a-45b6-be63-4e5f8035a3ce/Files/Groceries/instacart_orders\")\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"2050ba22-a5da-4318-9b67-74fd51a9cf1f","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-13T23:24:57.4964471Z","session_start_time":null,"execution_start_time":"2023-06-13T23:25:36.5988284Z","execution_finish_time":"2023-06-13T23:25:45.1548193Z","spark_jobs":{"numbers":{"SUCCEEDED":8,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4490,"rowCount":50,"jobId":78,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:42.595GMT","completionTime":"2023-06-13T23:25:42.618GMT","stageIds":[107,108,106],"jobGroup":"11","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4490,"dataRead":6557,"rowCount":61,"jobId":77,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:42.159GMT","completionTime":"2023-06-13T23:25:42.571GMT","stageIds":[104,105],"jobGroup":"11","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":6557,"dataRead":7480,"rowCount":22,"jobId":76,"name":"toString at String.java:2994","description":"Delta: Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...: Compute snapshot for version: 0","submissionTime":"2023-06-13T23:25:41.991GMT","completionTime":"2023-06-13T23:25:42.037GMT","stageIds":[103],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":75,"name":"","description":"Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:41.592GMT","completionTime":"2023-06-13T23:25:41.592GMT","stageIds":[],"jobGroup":"11","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":33824295,"dataRead":109516100,"rowCount":6842166,"jobId":74,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:37.719GMT","completionTime":"2023-06-13T23:25:41.486GMT","stageIds":[102],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":73,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:37.142GMT","completionTime":"2023-06-13T23:25:37.161GMT","stageIds":[100,101],"jobGroup":"11","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3500,"dataRead":524288,"rowCount":160,"jobId":72,"name":"getRowsInJsonString at Display.scala:403","description":"Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:36.910GMT","completionTime":"2023-06-13T23:25:37.124GMT","stageIds":[99],"jobGroup":"11","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":71,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 11:\n# Welcome to your new notebook\n# Type here in the cell editor to add code!\n# Azure storage access info\nblob_account_name = \"fabrictrainingcoursesa\"\nblob_container_name = \"coursedata\"\nblob_relative_path = \"instacart-orders.csv\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\n\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet/CSV, note that it won't load any data yet by now\n#df = spark.read.parquet(wasbs_path)\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)\nprint('Register the DataFrame as a SQL temporary view: source')\ndf.createOrReplaceTempView('source')\n\n# Display top 10 rows\nprint('Displaying top 10 rows: ')\ndisplay(spark.sql('SELECT * FROM source LIMIT 10'))\n\n# Save the dataframe into a parquet file to our data lakehouse...","submissionTime":"2023-06-13T23:25:36.690GMT","completionTime":"2023-06-13T23:25:36.825GMT","stageIds":[98],"jobGroup":"11","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"1791fa08-09e8-4886-856a-efeada5483f9"},"text/plain":"StatementMeta(, 2050ba22-a5da-4318-9b67-74fd51a9cf1f, 11, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Remote blob path: wasbs://coursedata@fabrictrainingcoursesa.blob.core.windows.net/instacart-orders.csv\nRegister the DataFrame as a SQL temporary view: source\nDisplaying top 10 rows: \n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"242cfcf8-588c-4021-b42a-1b89049f47a6","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 242cfcf8-588c-4021-b42a-1b89049f47a6)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"9706acd9-4411-4292-9753-c5377c776304":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2","1":"33120","2":"1","3":"1","index":1},{"0":"2","1":"28985","2":"2","3":"1","index":2},{"0":"2","1":"9327","2":"3","3":"0","index":3},{"0":"2","1":"45918","2":"4","3":"1","index":4},{"0":"2","1":"30035","2":"5","3":"0","index":5},{"0":"2","1":"17794","2":"6","3":"1","index":6},{"0":"2","1":"40141","2":"7","3":"1","index":7},{"0":"2","1":"1819","2":"8","3":"1","index":8},{"0":"2","1":"43668","2":"9","3":"0","index":9},{"0":"3","1":"33754","2":"1","3":"1","index":10}],"schema":[{"key":"0","name":"order_id","type":"string"},{"key":"1","name":"product_id","type":"string"},{"key":"2","name":"add_to_cart_order","type":"string"},{"key":"3","name":"reordered","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"count","isStacked":false,"binsNumber":10}},"customOptions":{}}},"07afe3cb-b851-4a1e-bde4-ef74fa0e6c1f":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"prepared soups salads","index":1},{"0":"2","1":"specialty cheeses","index":2},{"0":"3","1":"energy granola bars","index":3},{"0":"4","1":"instant foods","index":4},{"0":"5","1":"marinades meat preparation","index":5},{"0":"6","1":"other","index":6},{"0":"7","1":"packaged meat","index":7},{"0":"8","1":"bakery desserts","index":8},{"0":"9","1":"pasta sauce","index":9},{"0":"10","1":"kitchen supplies","index":10}],"schema":[{"key":"0","name":"aisle_id","type":"string"},{"key":"1","name":"aisle","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10}},"customOptions":{}}},"20ee5d39-8420-4b4c-9171-315e0e8f3d4d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"49302","2":"1","3":"1","index":1},{"0":"1","1":"11109","2":"2","3":"1","index":2},{"0":"1","1":"10246","2":"3","3":"0","index":3},{"0":"1","1":"49683","2":"4","3":"0","index":4},{"0":"1","1":"43633","2":"5","3":"1","index":5},{"0":"1","1":"13176","2":"6","3":"0","index":6},{"0":"1","1":"47209","2":"7","3":"0","index":7},{"0":"1","1":"22035","2":"8","3":"1","index":8},{"0":"36","1":"39612","2":"1","3":"0","index":9},{"0":"36","1":"19660","2":"2","3":"1","index":10}],"schema":[{"key":"0","name":"order_id","type":"string"},{"key":"1","name":"product_id","type":"string"},{"key":"2","name":"add_to_cart_order","type":"string"},{"key":"3","name":"reordered","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"count","isStacked":false,"binsNumber":10}},"customOptions":{}}},"242cfcf8-588c-4021-b42a-1b89049f47a6":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2539329","1":"1","2":"prior","3":"1","4":"2","5":"08","6":"NULL","index":1},{"0":"2398795","1":"1","2":"prior","3":"2","4":"3","5":"07","6":"15.0","index":2},{"0":"473747","1":"1","2":"prior","3":"3","4":"3","5":"12","6":"21.0","index":3},{"0":"2254736","1":"1","2":"prior","3":"4","4":"4","5":"07","6":"29.0","index":4},{"0":"431534","1":"1","2":"prior","3":"5","4":"4","5":"15","6":"28.0","index":5},{"0":"3367565","1":"1","2":"prior","3":"6","4":"2","5":"07","6":"19.0","index":6},{"0":"550135","1":"1","2":"prior","3":"7","4":"1","5":"09","6":"20.0","index":7},{"0":"3108588","1":"1","2":"prior","3":"8","4":"1","5":"14","6":"14.0","index":8},{"0":"2295261","1":"1","2":"prior","3":"9","4":"1","5":"16","6":"0.0","index":9},{"0":"2550362","1":"1","2":"prior","3":"10","4":"4","5":"08","6":"30.0","index":10}],"schema":[{"key":"0","name":"order_id","type":"string"},{"key":"1","name":"user_id","type":"string"},{"key":"2","name":"eval_set","type":"string"},{"key":"3","name":"order_number","type":"string"},{"key":"4","name":"order_dow","type":"string"},{"key":"5","name":"order_hour_of_day","type":"string"},{"key":"6","name":"days_since_prior_order","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["3"],"aggregationType":"count","isStacked":false,"binsNumber":10}},"customOptions":{}}}}},"trident":{"lakehouse":{"default_lakehouse":"a9f02573-715a-45b6-be63-4e5f8035a3ce","known_lakehouses":[{"id":"a9f02573-715a-45b6-be63-4e5f8035a3ce"}],"default_lakehouse_name":"Groceries_lh","default_lakehouse_workspace_id":"b12ab912-c7ea-4752-9acf-910618765439"}}},"nbformat":4,"nbformat_minor":0}